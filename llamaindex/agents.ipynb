{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5ae25a",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "- LlamaIndex supports 3 main types of reasoning agents:\n",
    "  1. Function Calling Agents: These work with AI models that can call specific functions.\n",
    "  2. ReAct Agents: These can work with any AI that does chat or text endpoints and deal with complex reasoning tasks.\n",
    "  3. Advanced Custom Agents: These use more compplex methods to deal with more complex tasks and workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1313d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.tools import FunctionTool\n",
    "def multiply(a:int,b:int)->int:\n",
    "    '''Multiply two integers and returns the resulting integer'''\n",
    "    return a*b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09216bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "agent=AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72e82e",
   "metadata": {},
   "source": [
    "- Agents are by default stateles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859b241e",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:628\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    629\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:369\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\multi_agent_workflow.py:399\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    397\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    400\u001b[39m     ctx,\n\u001b[32m    401\u001b[39m     ev.input,\n\u001b[32m    402\u001b[39m     tools,\n\u001b[32m    403\u001b[39m     memory,\n\u001b[32m    404\u001b[39m )\n\u001b[32m    406\u001b[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\react_agent.py:101\u001b[39m, in \u001b[36mReActAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    100\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    102\u001b[39m     raw = (\n\u001b[32m    103\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:462\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    461\u001b[39m cur_index = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat_completion(\n\u001b[32m    463\u001b[39m     messages=\u001b[38;5;28mself\u001b[39m._to_huggingface_messages(messages),\n\u001b[32m    464\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    465\u001b[39m     **model_kwargs,\n\u001b[32m    466\u001b[39m ):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].finish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:964\u001b[39m, in \u001b[36mAsyncInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    957\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    958\u001b[39m     inputs=messages,\n\u001b[32m    959\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    963\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_post(request_parameters, stream=stream)\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:290\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:276\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1161\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   1164\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m   1165\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m   1166\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m   1167\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response=\u001b[38;5;28;01mawait\u001b[39;00m agent.run(\u001b[33m\"\u001b[39m\u001b[33mWhat is 2 times 2?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Context\n\u001b[32m      3\u001b[39m ctx= Context(agent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:408\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    406\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    412\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:637\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    638\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    641\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    642\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    643\u001b[39m     )\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "response=await agent.run(\"What is 2 times 2?\")\n",
    "from llama_index.core.workflow import Context\n",
    "ctx= Context(agent)\n",
    "response=await agent.run(\"My nname is Bob.\",ctx=ctx)\n",
    "response=await agent.run(\"What was my name again?\",ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3c13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "db=chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection=db.get_or_create_collection(\"alfred\")\n",
    "vector_store=ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "pipeline=IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25,chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b036460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index=VectorStoreIndex.from_vector_store(vector_store,embed_model=embed_model)\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "query_engine=index.as_query_engine(llm=llm,similarity_top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c3de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tool=QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"name\",\n",
    "    description=\"a specific description\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent=AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d40d71",
   "metadata": {},
   "source": [
    "# Multi-agent systems\n",
    "\n",
    "- by giving each agent a name and description, the system maintains a single active speaker, with each agent having the ability to hand off to another agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a80b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:628\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    629\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:369\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\multi_agent_workflow.py:399\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    397\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    400\u001b[39m     ctx,\n\u001b[32m    401\u001b[39m     ev.input,\n\u001b[32m    402\u001b[39m     tools,\n\u001b[32m    403\u001b[39m     memory,\n\u001b[32m    404\u001b[39m )\n\u001b[32m    406\u001b[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\react_agent.py:101\u001b[39m, in \u001b[36mReActAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    100\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    102\u001b[39m     raw = (\n\u001b[32m    103\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:462\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    461\u001b[39m cur_index = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat_completion(\n\u001b[32m    463\u001b[39m     messages=\u001b[38;5;28mself\u001b[39m._to_huggingface_messages(messages),\n\u001b[32m    464\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    465\u001b[39m     **model_kwargs,\n\u001b[32m    466\u001b[39m ):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].finish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:964\u001b[39m, in \u001b[36mAsyncInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    957\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    958\u001b[39m     inputs=messages,\n\u001b[32m    959\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    963\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_post(request_parameters, stream=stream)\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:290\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:276\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1161\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   1164\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m   1165\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m   1166\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m   1167\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     39\u001b[39m agent = AgentWorkflow(\n\u001b[32m     40\u001b[39m     agents=[calculator_agent, query_agent], root_agent=\u001b[33m\"\u001b[39m\u001b[33mcalculator\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Run the system\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(user_msg=\u001b[33m\"\u001b[39m\u001b[33mCan you add 5 and 3?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:408\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    406\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    412\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:637\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    638\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    641\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    642\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    643\u001b[39m     )\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent,\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "query_agent = ReActAgent(\n",
    "    name=\"info_lookup\",\n",
    "    description=\"Looks up information about XYZ\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent, query_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await agent.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16025abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingfaceagents-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
