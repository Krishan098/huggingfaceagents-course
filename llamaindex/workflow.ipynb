{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bb17a0",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "- A workflow provides a structured way to organize your code into sequential and manageable steps.\n",
    "- created by defining Steps which are triggered by Events, and themselves emit Events to trigger further steps.\n",
    "\n",
    "##### Workflows offer several key benefits:\n",
    "\n",
    "    - Clear organization of code into discrete steps\n",
    "    - Event-driven architecture for flexible control flow\n",
    "    - Type-safe communication between steps\n",
    "    - Built-in state management\n",
    "    - Support for both simple and complex agent interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357b425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import StartEvent,StopEvent,Workflow,step\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self,ev:StartEvent)->StopEvent:\n",
    "        return StopEvent(result=\"Hello World\")\n",
    "w=MyWorkflow(timeout=10,verbose=False)\n",
    "result=await w.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50e545",
   "metadata": {},
   "source": [
    "## Connecting Multiple Steps\n",
    "\n",
    "- we create custom events that carry data between steps.\n",
    "- we add an event that is passed between the steps and transfer the output of the first step to the second step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e314ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result:str\n",
    "class MultiStepWorkFlow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self,ev:StartEvent)->ProcessingEvent:\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "    @step\n",
    "    async def step_two(self,ev: ProcessingEvent)->StopEvent:\n",
    "        final_result=f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "w=MultiStepWorkFlow(timeout=10,verbose=False)\n",
    "result= await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34de4f",
   "metadata": {},
   "source": [
    "## Loops and Branches\n",
    "\n",
    "- The type hinting is the most powerful part of workflows because it allows us to create branches, loops and joins to facilitate more complex workflows.\n",
    "- LoopEvent is taken as input for the step and can also be returned as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a05fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad thing happened\n",
      "Bad thing happened\n",
      "Bad thing happened\n",
      "Bad thing happened\n",
      "Good thing happened\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a73186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "#w = ... # as defined in the previous section\n",
    "draw_all_possible_flows(w, \"flow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c295e1",
   "metadata": {},
   "source": [
    "## State Management\n",
    "\n",
    "- It is useful when you want to keep track of the state of the workflow, so that every step has access to the same state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d325fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context,StartEvent,StopEvent\n",
    "class stateManagementWorkflow(Workflow):\n",
    "    @step\n",
    "    async def query(self,ctx:Context,ev:StartEvent)->StopEvent:\n",
    "        await ctx.set(\"query\",\"What is the capital of France?\" )\n",
    "        val=ctx.event_params\n",
    "        query=await ctx.get(\"query\")\n",
    "        return StopEvent(result=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9bf15",
   "metadata": {},
   "source": [
    "## Automating workflows with Multi-agent workflows\n",
    "\n",
    "- We can use the AgentWorkflow class to create a multi-agent workflow.\n",
    "- The AgentWorkflow uses Workflow Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities.\n",
    "- One agent must be designated as the root agent in the AgentWorkflow constructor. When a user message comes in, it is first routed to the root agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fafafc",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:628\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    629\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:369\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\multi_agent_workflow.py:399\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    397\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    400\u001b[39m     ctx,\n\u001b[32m    401\u001b[39m     ev.input,\n\u001b[32m    402\u001b[39m     tools,\n\u001b[32m    403\u001b[39m     memory,\n\u001b[32m    404\u001b[39m )\n\u001b[32m    406\u001b[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\react_agent.py:101\u001b[39m, in \u001b[36mReActAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    100\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    102\u001b[39m     raw = (\n\u001b[32m    103\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:462\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    461\u001b[39m cur_index = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat_completion(\n\u001b[32m    463\u001b[39m     messages=\u001b[38;5;28mself\u001b[39m._to_huggingface_messages(messages),\n\u001b[32m    464\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    465\u001b[39m     **model_kwargs,\n\u001b[32m    466\u001b[39m ):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].finish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:964\u001b[39m, in \u001b[36mAsyncInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    957\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    958\u001b[39m     inputs=messages,\n\u001b[32m    959\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    963\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_post(request_parameters, stream=stream)\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:290\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:276\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1161\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   1164\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m   1165\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m   1166\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m   1167\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     33\u001b[39m workflow = AgentWorkflow(\n\u001b[32m     34\u001b[39m     agents=[multiply_agent, addition_agent],\n\u001b[32m     35\u001b[39m     root_agent=\u001b[33m\"\u001b[39m\u001b[33mmultiply_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Run the system\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m workflow.run(user_msg=\u001b[33m\"\u001b[39m\u001b[33mCan you add 5 and 3?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:408\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    406\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    412\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:637\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    638\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    641\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    642\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    643\u001b[39m     )\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8ddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "async def add(ctx:Context,a:int,b:int)->int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    cur_state=await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"]+=1\n",
    "    await ctx.set(\"state\",cur_state)\n",
    "    return a+b\n",
    "async def multiply(ctx:Context,a:int,b:int)->int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    cur_state=await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"]+=1\n",
    "    await ctx.set(\"state\",cur_state)\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c12000",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:628\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    629\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:369\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\multi_agent_workflow.py:399\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    397\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    400\u001b[39m     ctx,\n\u001b[32m    401\u001b[39m     ev.input,\n\u001b[32m    402\u001b[39m     tools,\n\u001b[32m    403\u001b[39m     memory,\n\u001b[32m    404\u001b[39m )\n\u001b[32m    406\u001b[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\workflow\\react_agent.py:101\u001b[39m, in \u001b[36mReActAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    100\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    102\u001b[39m     raw = (\n\u001b[32m    103\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\llms\\huggingface_api\\base.py:462\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    461\u001b[39m cur_index = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat_completion(\n\u001b[32m    463\u001b[39m     messages=\u001b[38;5;28mself\u001b[39m._to_huggingface_messages(messages),\n\u001b[32m    464\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    465\u001b[39m     **model_kwargs,\n\u001b[32m    466\u001b[39m ):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].finish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:964\u001b[39m, in \u001b[36mAsyncInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    957\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    958\u001b[39m     inputs=messages,\n\u001b[32m    959\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    963\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_post(request_parameters, stream=stream)\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:290\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:276\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1161\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   1164\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m   1165\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m   1166\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m   1167\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m workflow=AgentWorkflow(\n\u001b[32m      2\u001b[39m     agents=[multiply_agent,addition_agent],\n\u001b[32m      3\u001b[39m     root_agent=\u001b[33m\"\u001b[39m\u001b[33mmultiply_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     initial_state={\u001b[33m\"\u001b[39m\u001b[33mnum_fn_calls\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0\u001b[39m},\n\u001b[32m      5\u001b[39m     state_prompt=\u001b[33m\"\u001b[39m\u001b[33mCurrent state:\u001b[39m\u001b[38;5;132;01m{state}\u001b[39;00m\u001b[33m.User message\u001b[39m\u001b[38;5;132;01m{msg}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m ctx=Context(workflow)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response=\u001b[38;5;28;01mawait\u001b[39;00m workflow.run(user_msg=\u001b[33m\"\u001b[39m\u001b[33mCan you add 5 and 3?\u001b[39m\u001b[33m\"\u001b[39m,ctx=ctx)\n\u001b[32m      9\u001b[39m state=\u001b[38;5;28;01mawait\u001b[39;00m ctx.get(\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(state[\u001b[33m\"\u001b[39m\u001b[33mnum_fn_calls\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:408\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    406\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    412\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\9\\huggingfaceagents-course\\.venv\\Lib\\site-packages\\llama_index\\core\\workflow\\context.py:637\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    638\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    641\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    642\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    643\u001b[39m     )\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "workflow=AgentWorkflow(\n",
    "    agents=[multiply_agent,addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    "    initial_state={\"num_fn_calls\":0},\n",
    "    state_prompt=\"Current state:{state}.User message{msg},\"\n",
    ")\n",
    "ctx=Context(workflow)\n",
    "response=await workflow.run(user_msg=\"Can you add 5 and 3?\",ctx=ctx)\n",
    "state=await ctx.get(\"state\")\n",
    "print(state[\"num_fn_calls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a61c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingfaceagents-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
